Metadata-Version: 2.4
Name: gpu-agent-opt
Version: 0.1.1
Summary: AI agent framework for GPU kernel autotuning and optimization
Author-email: Atul Vaish <admin@aifusion.in>
License: MIT
Project-URL: Homepage, https://github.com/intelav/gpu_agent_opt
Project-URL: Repository, https://github.com/intelav/gpu_agent_opt
Project-URL: Issues, https://github.com/intelav/gpu_agent_opt/issues
Requires-Python: >=3.8
Description-Content-Type: text/markdown
License-File: LICENSE
Requires-Dist: numpy
Requires-Dist: torch
Requires-Dist: rasterio
Requires-Dist: tqdm
Requires-Dist: tabulate
Requires-Dist: scikit-optimize
Requires-Dist: cloudpickle
Requires-Dist: nvtx
Dynamic: license-file

# 🧠 **gpu-agent-opt**

**Unified AI Agent Framework for GPU Kernel Autotuning, Scientific Computing, and CUDA Exploration**

`gpu-agent-opt` is an evolving Python package designed to **orchestrate AI agents** for **Triton, CUDA, CuPy, cuDF**, and advanced GPU programming patterns — combining **automatic kernel generation**, **profiling**, and **optimization** with a knowledge-driven loop:

👉 **Sense → Think → Act → Learn**

The long-term goal is to make this a **one-stop GPU research & optimization layer**: integrating deep learning graph compilers (PyTorch Inductor / XLA), scientific computing (CuPy / cuDF), and low-level CUDA primitives (e.g., coalesced memory, warp shuffle, tensor cores) into a single **agentic optimization system**.

---

## ✨ **Core Capabilities**

- 🧠 **AI Agent Kernel Optimizer**  
  - Automatic search & tuning of kernel launch configurations (blockDim, gridDim, stride, etc.)  
  - Bayesian optimization / RL-driven exploration for both Triton and CUDA kernels.

- 🧪 **Multi-Backend Support**  
  - ✅ **Triton Kernels** (via PyTorch Inductor or custom)  
  - ✅ **Raw CUDA kernels** (via NVRTC / PyCUDA / C++ extensions)  
  - ✅ **CuPy & cuDF** for scientific array/dataframe computing  
  - Future: integrate CUDA Graphs, Cooperative Groups, tensor cores, async copies, and MIG.

- 🔬 **Profiler Integration**  
  - Seamless wrapping of **Nsight Compute**, **Nsight Systems**, and **PyTorch profiler** for real GPU metrics (SM utilization, DRAM throughput, kernel fusion, memory stalls).

- 📚 **Knowledge Base**  
  - Store and reuse best kernel configs per GPU architecture.  
  - Capture performance signatures across RTX, Jetson, and HPC GPUs (A100, H100, etc.).

- 🛰 **Target Use Cases**  
  - Geospatial AI auto-annotation pipelines (DINOv2, SAM2, YOLO, NDWI/LBP preprocessing)  
  - Deep learning inference/training acceleration through Inductor + custom kernels  
  - Scientific/HPC workloads (FFT, FDTD3D, conjugate gradient, Monte Carlo, etc.)  
  - CUDA educational benchmarking (transpose, reduction, memory hierarchy, etc.)  
  - Edge AI autotuning for embedded GPUs.

---

## 🔥 **CUDA Samples Integration**

The agent aims to provide a **Pythonic exploration layer over all classic CUDA patterns**, using the official CUDA Samples as a baseline:

- **Memory & Data Movement**:  
  - `bandwidthTest`, `transpose` (coalesced memory), `globalToShmemAsyncCopy`, `simpleZeroCopy`, `UnifiedMemoryStreams`.

- **Computation Kernels**:  
  - `reduction` (multi-block, warp shuffle), `scan`, `radixSortThrust`, `matrixMul`, tensor core GEMM (bf16, tf32).

- **Advanced Features**:  
  - CUDA Graphs (`simpleCudaGraphs`, `graphMemoryFootprint`), Cooperative Groups (`binaryPartitionCG`, `shfl_scan`), Async API, stream priorities, system-wide atomics.

- **Linear Algebra & Solvers**:  
  - cuBLAS, cuSolver (LU, QR, Cholesky), conjugate gradient multi-GPU variants.

- **Signal & Image Processing**:  
  - FFT (CUFFT 1D/2D/MGPU), DCT, histogram, Sobel filters, NPP routines.

- **Misc / Educational**:  
  - `deviceQuery`, `inlinePTX`, `cudaOpenMP`, NVRTC runtime compilation, occupancy calculators.

All these are being **systematically wrapped into Python interfaces** and exposed to the **KernelAgent** for exploration, profiling, and hybrid fusion with Triton / CuPy / Inductor.

---

## 🧠 **Example: Autotuning a Triton or CUDA Kernel**

```python
from gpu_agent_opt import KernelAgent
import cupy as cp

# Example: Autotuning a custom transpose kernel (Triton or raw CUDA)
kernel_code = r"""
extern "C" __global__ void transpose(float *odata, float *idata, int width, int height) {
    __shared__ float tile[32][33]; // coalesced read + avoid bank conflicts
    int x = blockIdx.x * 32 + threadIdx.x;
    int y = blockIdx.y * 32 + threadIdx.y;
    if (x < width && y < height) tile[threadIdx.y][threadIdx.x] = idata[y * width + x];
    __syncthreads();
    x = blockIdx.y * 32 + threadIdx.x;
    y = blockIdx.x * 32 + threadIdx.y;
    if (x < height && y < width) odata[y * height + x] = tile[threadIdx.x][threadIdx.y];
}
"""

agent = KernelAgent(kernel_src=kernel_code, backend="cuda")
data = cp.random.rand(1024, 1024).astype(cp.float32)
best_cfg, results = agent.autotune(
    search_space={"block_size": [16, 32], "grid_size": [32, 64]},
    input=data
)
print(best_cfg)
```

---

## 🧪 **Scientific + DL Interoperability**

- CuPy / cuDF kernels can be **fused with Triton / CUDA kernels** in the same agent pipeline.  
- PyTorch Inductor graphs can be **partially compiled** and partially replaced by hand-tuned kernels.  
- Target: seamlessly combine high-level deep learning graphs with low-level HPC kernels.

---

## 📦 Installation

Coming soon on PyPI:  
👉 [https://test.pypi.org/project/gpu-agent-opt/](https://test.pypi.org/project/gpu-agent-opt/0.1.0/)

```bash
pip install gpu-agent-opt
```

Development install:
```bash
git clone https://github.com/yourusername/gpu_agent_opt.git
cd gpu_agent_opt
pip install -e .
```

---

## 📊 Roadmap

- ✅ Triton kernel integration  
- ✅ PyTorch Inductor graph fusion exploration  
- 🚧 CuPy / cuDF integration for scientific computation  
- 🚧 CUDA Samples porting (transpose, reduction, etc.)  
- 🚧 Tensor Core autotuning (bf16, tf32)  
- 🚧 Multi-GPU / MIG profiling and scheduling  
- 🚧 Web dashboard for kernel search spaces & profiling results

---

## 🧪 Tutorials (Coming Soon)

| Tutorial | Description |
|----------|-------------|
| Transpose Coalescing | Explore memory coalescing & shared memory tiling |
| Reduction with Warp Shuffle | Implement warp-level primitives & tune performance |
| Tensor Core Autotuning | Use BF16/TF32 GEMM samples to autotune tensor cores |
| Hybrid Graph Fusion | Combine Inductor subgraphs with custom Triton kernels |
| Scientific Kernels | FFT, DCT, Monte Carlo, conjugate gradient solvers |

---

## 🤝 Contributing

Contributions are very welcome, especially for:
- Wrapping additional CUDA samples into Python bindings
- Adding new search strategies (RL, evolutionary)
- Expanding scientific kernel coverage (FFT, solvers, etc.)
- Profiling backends (Nsight Compute scripting, CUPTI)

👉 Open issues & PRs on [GitHub](https://github.com/yourusername/gpu_agent_opt)

---

## 📜 License

MIT License — see [LICENSE](LICENSE)
